<html>
	<head>

		<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
		<script id="MathJax-script" async
			src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
		</script>

		<link rel="shortcut icon" href="images/icon.ico">
		<style type="text/css">
body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: center; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}

	.main-content-block {
		width: 80%; /* Change this percentage as needed */
    	max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 30px 8px 30px;
		font-family: 'Garamond', serif;
		line-height: 1.5;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: 'Garamond', serif;
			line-height: 1.5;
			padding: 5px;
	}
	.margin-right-block {
			font-family: 'Garamond', serif;
			line-height: 1.5;
			font-size: 14px;
			width: 5%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			/* margin: 0; */
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
		padding: 5px 0px;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

	.caption {
		text-align: center; 
		margin-top: 10px;
		font-size: 16px;
		font-style: italic;
	}

	video {
		margin: 0;
	}

	.media-container {
		display: flex; 
		justify-content: center; 
		align-items: center; 
		gap: 0;	
	}
	
	.underl {
		text-decoration: underline;
		vertical-align: baseline;
	}


	thead tr {
		border-bottom: 2px solid black; /* Makes the line at the bottom of thead thicker */
	}

		</style>

		<title>LOFT: Learned Open Set Flow-based Tracking</title>
		<meta property="og:title" content="LOFT: Learned Open Set Flow-based Tracking" />
		<meta charset="UTF-8">
	</head>

	<body>

		<div class="content-margin-container">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">
				<table class="header" align=left style="margin: 30px 0px 0px 0px;">
					<tr>
						<span style="font-size: 32px; font-family: 'Garamond', serif;">
							 LOFT:  <span class="overL">L</span>earned <span class="overL">O</span>pen <span class="overL">F</span>low-based <span class="overL">T</span>racking
						</span>

					</tr>
					<tr>
						<td align=left>
							<span style="font-size:17px"><a href="index.html">Qingyuan Li</a></span>
						</td>
						<td align=left>
							<span style="font-size:17px"><a href="index.html">Alexandru Luchianov</a></span>
						</td>
						<tr>
							<td colspan=4 align=left><span style="font-size:18px">Final project for MIT 6.7960</span></td>
						</tr>
				</table>
			</div>
			<div class="margin-right-block">
			</div>
		</div>

		<div class="content-margin-container" id="preface">
			<div class="margin-left-block">
				<!-- table of contents here -->
				<div style="position:fixed; max-width:inherit; top:max(20%,120px); font-size:16px;">
					<b style="font-size:16px">Outline</b><br><br>
					<a href="#preface">Preface</a><br><br>
					<a href="#motivation">Motivation</a><br><br>
					<a href="#related">Related works</a><br><br>
					<a href="#inputs">Model inputs</a><br><br>
					<a href="#unet">U-Net</a><br><br>
					<a href="#dataset">Dataset Collection</a><br><br>
					<a href="#metrics">Metrics</a><br><br>
					<a href="#evaluation">Evaluation</a><br><br>
					<a href="#conclusion">Conclusion</a><br><br>
					<a href="#codebase">Codebase</a><br><br>
					<a href="#citations">References</a><br><br>
					<a href="#appendix">Appendix</a><br><br>
				</div>
			</div>
			<div class="main-content-block">
			</div>
			<div class="margin-right-block">
			</div>
		</div>

		<div class="content-margin-container" id="preface">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">
				<figure>
					<img src="./images/main.png">

					<div class="caption">
						Example frame from our algorithm. Outputs from top left, clockwise: RGB image with annotated dynamic objects, depth image, raw model output mask, post-processed model output mask.
					</div>
				</figure>
				<br>
				<h1>Preface </h1>
				One of the <a href='https://liqyn.github.io/dynamic-object-detection/floros/'>previous projects of Qingyuan Li</a> involved dynamic object tracking by using the flow residual information, however the result was lacking.
				This section aims to provide context as to what the previous approach did, why it failed and why deep learning is the solution.

				<p> In very succinct terms, the core pipeline of the previous project could be described as such:
				<ol>
					<li> Gather data for the dynamic object tracking experiment. </li>
					<li> Extract the optical flow from the image using a known model(RAFT) </li>
					<li> Process the optical flow and the original frame to obtain the objects </li>
				</ol>

				The idea seems simple in practice, so simple, in fact, that this project could also be described by a similar list of steps.
				However, "processing the optical flow" hides a lot of abstraction underneath. In the previous project, this processing meant tagging every pixel 
				that is moving faster than a certain velocity as a "dynamic object" and then merely post-processing the binary mask with OpenCV morphological operations in order to remove the noise.
				This game of post-processing was akin to Whac-a-mole, opening an image would remove small points of noise, closing would remove small holes, but the noise would still be noticeable. (See Video 1 for an example.)
				The winning strategy of this game was, obviously, not to play at all. For this project, we decided that a model is much better suited at learning what set of transformations would be best.

				<figure>
					<video class='my-video' loop autoplay muted controls style="width: 800px; max-width: 100%;">
						<source src="./videos/non_learned.mp4" type="video/mp4">
					</video>

					<div class="caption">
					<span>Video 1. Example output sequence from the evaluation dataset with the non-learned algorithm</span>
					</div>
				</figure>

				<p>Unfortunately, replacing the manual transformation of data by a learning model subtly (and signficantly) changes the other steps of the pipeline too. 
				The most significant change was that of what ground truth is necessary. In the previous iteration of the project our evaluation metric was merely whether the detected location of the object was close enough
				to where we know the object to be. However, our segmentation model requires the whole segmentation of the object as a ground truth.

				<p>We could not collect this ground truth as we were only able to collect data about the approximate pose of the objects.
				Note that if we had the shape of the objects, we would have been able to generate the ground truth by simply projecting them. 
				Our solution is to generate the ground truth using the Segment Anything model (<a href="#ref_2">[2]</a>) and then train our smaller model on that.
				Note that training on this synthetic data makes our model inherently less accurate than SAM, however our model is much lighter.

				<p>
			</div>
			<div class="margin-right-block">
			</div>
		</div>

		<div class="content-margin-container" id="motivation">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">

				<h1>Motivation</h1>

				Unmanned Aerial Vehicles and Unmaned ground vehicles have numerous applications in fields such as delivery, photography, monitoring and many others. 
				However, being unmanned leaves them dependent on signal coverage and a stable connection in order to be teleoperated by an operator.
				In order to overcome this limitation, much effort has been directed towards the issue of autonomous control.

				<p> Unfortunately, autonomous control has its own set of limitations. Where a human would be able to operate with only limited signal,
				an autonomous device often requires significantly more sensors. For example, a Waymo car employs a total of 29 cameras and possesses Lidar capabilities.
				This extensive payload is the reason why adoption is limited even for low-risk applications. 

				<p> We aim to provide a lightweight solution for performing real-time dynamic obstacle recognition. 
		
			</div>
			<div class="margin-right-block">
			</div>
		</div>

		<div class="content-margin-container" id="related">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">

				<h1>Related works</h1>

				Segmentation is an extremely broad field that has spawned many diverse tasks. 
				In order to tackle the ever-expanding nature of the field, significant time and effort has been invested in creating capable models that can adapt to many of these existing tasks.
				One of the most prominent and succesful examples of such a project is the Segment Anything (<a href="#ref_2">[2]</a>) model. Naturally, the model's core strength, its generality, is also its core weakness 
				as it cannot properly take advantage of highly specific advantages. To put this into our context of Video Object Segmentation, SAM(Segment Anything Model) cannot take advantage of the information that is inherent in the video format 
				and must segment each frame independently.

				<p> A more recent work that tries to bridge this gap is <a href="#ref_1">[1]</a>. Instead of ignoring the information from the video feed itself, it tries to feed the optical flow either as an input (by converting it to an RGB image) 
				or to create prompts for the original image.
				While this approach does not discard the useful data from the video, it achieves its purpose in a roundabout way that avoid training a new model and thus not really specializing.
				Another significant disadvantage acknowledged by the authors of the paper: SAM is computationally heavy, thus the resulting model is also extremely slow.
				In contrast, our model is trained and specialized, thus is can be much lighter and efficient.

				<p> <a href="#ref_4">[4]</a> and <a href="#ref_3">[3]</a> further generalize the SAM model, thus allowing it to to process videos. Note that this advancement obsoletes the papers that merely provide a wrapper around SAM 
				in order to provide it with additional video capabilities. Unfortunately, SAM is still a massive model that is extremely computationally expensive and thus impractical for our purposes. However, we do note
				that it is still a key component of our pipeline that allows us to cheaply and reliably generate ground truths for our collected training data. 

				<p> <a href="#ref_10">[10]</a> further augments SAM by providing semantic context from DINO(<a href="#ref_11">[11]</a>) features.
				While this approach achieves really high accuracy, that is achieved at the cost of speed. We considered using DINO features with our model, however we ultimately decided against it 
				due to DINO feature vectors being really long and not compatible with our idea of creating a lightweight model.

				<p>An example of a lightweight model that makes use of optical flow is RST-MODNet (<a href="#ref_9">[9]</a>).
				This model uses a LSTM-based network in order to leverage the temporal information to a stronger degree. 
				The disadvantage of this approach is that the recurrent nature of the network makes it really computationally expensive to train.
				Another significant difference is that they use the raw optical flow, rather than the residual flow which accounts for the movement of the observer.

				<p> We note that most of these works build upon image segmentation towards video segmentation and that advacements in the former also tend to provide advancements in the latter.
				However, this relationship is not unidirectional and sometimes advancements in video segmentation can provide advancements in image.
				<a href="#ref_5">[5]</a> provides a framework for generating synthetic flows from depth data. This enables models that are reliant on optical flow cues to provide similar results when tested on static images.
				Specifically, this implies that our work can be further generalized to static images.
			
			</div>
			<div class="margin-right-block">
			</div>
		</div>

		<div class="content-margin-container" id="inputs">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">

				<h1>Model inputs</h1>

				As one of the main ingredients, it is natural that we dedicate some of our attention to the problem of optical flow. 
				Due to it being a relatively common problem there exist pretty performant solutions such as RAFT (<a href="#ref_6">[6]</a>). We additionally remark that not only is RAFT one of the solutions, 
				it is the primary solution to the problem of optical flow, being at the base of most of the related works. Even though optical flow can be summarized roughly as "how did a pixel move between frames" and then used as blackbox,
				we consider that some context about this "black box" could help us make an informed decision about the rest of our architecture.

				<p> Formally, optical flow solves the below optimization problem:
				\[ E = \min_{\mu} \left[ (\nabla^T \mathbf{I}_1\mu - (\mathbf{I}_2-\mathbf{I}_1)^2+\alpha(|\nabla \mu_x|^2+|\nabla \mu_y|^2)) \right] \]

				<p> There are two core assumptions that motivate the above formulation:
				<ol>
					<li> <strong>Color constancy:</strong> Pixels won't spontaneously change colors. </li>
					<li> <strong>Velocity averaging: </strong> Nearby pixels move together. </li>
				</ol>

				<p> Obviously, these assumptions are not universal truths and one can construct scenarios in which they do no hold ranging from the most contrived
				(we are trying to follow a chameleon or every pixel has a mind of its own and running independently) 
				to quite mundane things (lighting conditions change or it is raining).
				In our case, we performed our data collection in an indoor environment where such anomalies can be controlled and minimized. 

				<p> A pitfall of the optimization problem formulation is that often we don't have only two adjacent frames, but rather an entire video. 
				Solving an optimization problem for each two adjacent frames would be extremely computationally expensive. RAFT circumvents expensive computation 
				by extracting per-pixel features and building multi-scale 4D correlation volumes. Aside from the efficiency gains, this representation confers it 
				some additional robustness against lighting changes and blur.

				<p> For the sake brevity, we will only provide a description of each of the model inputs. (For an actual derivation of them, please refer to the <a href="#appendix">appendix</a>):
				
				<ul>
				  <li>
				    <strong>RAFT Flows 2D</strong>: Flow of a pixel from the previous frame to the current frame as computed by RAFT
				  </li>
				  <li>
				    <strong>Geometric Flow 2D</strong>: Flow of a pixel if we assume that the background is static and we account only for the camera movement. 
				  </li>
				  <li>
				    <strong>Residual 2D</strong>: The difference between the RAFT flow and the geometric flow. This represents the actual movement in 2-dimensional space induced by the object, not by the camera.
				  <li>
				    <strong>RAFT Flows 3D</strong>: Flow of a pixel from the previous frame to the current frame as computed by RAFT plus the depth information.
				  </li>
				  <li>
				    <strong>Geometric Flows 3D</strong>: Flow of a pixel in 3-dimensional space if we assume that the background is static and we account only for the camera movement.
				  </li>
				  <li>
				    <strong>Residual Flow 3D</strong>: The difference between the RAFT flow and the geometric flow. This represents the actual movement in 3-dimensional space induced by the object, not by the camera.
				  </li>
				  <li>
				    <strong>Depth Data</strong>: Depth information for each pixel. Note that this is capped at 6m meters due to the limitations of our sensors.  
				  </li>
				  <li>
				    <strong>RGB Data</strong>: The raw RGB image.
				  </li>
				</ul>
				
				These inputs will be later fed into the U-Net in the form of an image with enough channels for all of the above.

		</div>
			<div class="margin-right-block">
			</div>
		</div>

		<div class="content-margin-container" id="unet">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">

				<h1> U-Net </h1>

				Image segmentation is a notoriously hard problem to annotate data for.
				Rather than trying to annotate an ever-increasing amount of data, some projects have opted for trying to make more efficient use of the existing annotated data.
				This has led to the creation of the U-Net (<a href="#ref_7">[7]</a>). Aside from their efficiency in terms of training data, these models are small, fast and reliable. 
				This makes them a natural choice of a base on which to build our custom model.

				<p> A succinct description of the U-net architecture is that it consists of a contracting path and an expansive path. 
				The contractive path reduces the resolution of the image by downsampling while adding more channels in order to propagate context.
				The expansive path does the exact opposite: it upsamples while reducing the number of channels.
				This leads the network to have a "U" structure, thus providing it with its name. The details of the paths are more easily apparent in 
				the visual reminder of the U-Net architecture that we also provide below in the form of Figure 1:


				<figure>
					<img src="./images/u-net-architecture.png"
					     alt="Traditional U-Net architecture diagram showing the encoder, decoder, and skip connections."
					     >

						 <div class="caption">
			  <span>Figure 1: The U-Net Architecture.</span>
			</div>
				</figure>

				The main distinction between the traditional architecture and ours is that instead of using a traditional RGB image, we instead add additional channels that provide additional context.
				This additional context is mostly either depth information or optical flow information. Note that optical flow itself also can be presented in many ways such as either 3-dimensional, 2-dimensional, RAFT flow or geometric flow.

				<p> We use the Dice coefficient in order to evaluate the outputs of the U-Net and train it. We note that this metric only measures the quality of the segmentation which is merely an intermediate product. 
				The overall metric that we use for dynamic object detection is different. We remind that the dice formula is 
				$$DSC = \frac{2|X \cap Y|}{|X| + |Y|}$$

				If our collected data was free of noise, then X would be the set of segmented pixels in the ground truth and Y would be set of segmented pixels in the output of our model. 
				However, due to imperfect collection of data some of our depth pixels were nan. In order to account for the errors of our equipment, we only included the unaffected pixels in this metric.

				<p> The specific U-Net implementation on which we based our work can be found here at <a href="#ref_8">[8]</a>.
		</div>
			<div class="margin-right-block">
			</div>
		</div>

		<div class="content-margin-container" id="dataset">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">

				<h1> Dataset colection </h1>

				We collected all of the training and evaluation data at the Highbay of the Aerospace Controls Laboratory at MIT. 
				This controlled environmnet provided us with stable lightning and weather conditions in order for the optical flow to be easy to detect accurately,
				and contains a ground truth motion capture system for evaluation.
				We remind that our aim is to measure the efficacy of the techniques applied for the processing of optical flow and image data, not to optimize optical flow detection itself. 

				<p> In terms of hardware, we used a RealSense D455 depth camera on a Clearpath Husky with an Ouster lidar and MicroStrain IMU running lidar-inertial odometry. The moving objects are teleoperated hexcopter and another ground robot.

				<p> The ground truth pose data was collected using a Vicon motion capture system and recorded in a ROS2(Robot Operating System) bag. The Husky recorded a ROS1 bag with depth camera and pose data. We use our own ROS bag processing software for dataloading.
				Note that the ground truth collected above only provided pose information. Ground truth pose information combined with a rought estimate of the shapes of the robots allowed us to compute approximate bounding boxes.
				This allowed us to use SAM(<a href="#ref_2">[2]</a>) with bounding box-guided prompts to generate ground truth for the image segmentation. 
		
			</div>
			<div class="margin-right-block">
			</div>
		</div>

		<div class="content-margin-container" id="metrics">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">

				<h1> Metrics </h1>

				We used the standard precision and recall metrics in order to quantify the performance of our model. 
				Note that since ground truth data and visual data are collected by different machines, there might exist discrepancies gaps in the collected timestamps. 
				In order to account for this, we only compute the metrics for frames for which there exists a close enough ground truth. Let the model return a list of detected dynamic objects and their poses.

				<p>We consider an object to be relevant only if we could feasibly detect it and the detected position is close to the true location (within one meter).
				By feasibly detect, we mean that we only consider objects that are both moving with high enough velocity (at least 0.1 m/s) and whose centroid lies within the boundary of the current image frame.

				Based on this definition of relevant we provide the following metrics:
				<ul>
					<li><strong>Precision:</strong> The ratio of returned objects that actually are relevant (actually exist and could be detected) </li>
					<li><strong>Recall:</strong> The ratio of relevant objects(that actually exist and could be detected) that were actually returned  </li>
				</ul>

				All evaluations were run with the RAFT Sintel model.
				Opening and closing with a 9x9 kernel were used as morphological operations for post-processing all binary masks to reduce point noise.
				Objects with a maximum standard deviation below 0.05m or above 1.0m are ignored. Objects beyond the depth range are also ignored.
			</div>
			<div class="margin-right-block">
			</div>
		</div>
		
		<div class="content-margin-container" id="evaluation">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">
				<h1> Evaluation </h1>

				<div class="caption">
			  <span>Table 1. Comparison of the inputs included for each one of the models</span>
			</div>

			<table border="1" style="width:100%; text-align:center; border-collapse:collapse;">
			  <thead>
			    <tr>
			      <th>Model</th>
			      <th>RAFT Flows 2D</th>
			      <th>Geometric Flow 2D</th>
			      <th>Residual 2D</th>
			      <th>RAFT Flows 3D</th>
			      <th>Geometric Flows 3D</th>
			      <th>Residual Flow 3D</th>
			      <th>Depth (Cur)</th>
			      <th>RGB (Cur)</th>
				  <th>Depth (Prev)</th>
			      <th>RGB (Prev)</th>
			    </tr>
			  </thead>
			  <tbody>
			    <tr>
			      <td>Non-learned</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
				  <td>&cross;</td>
			      <td>&cross;</td>
			    </tr>
				<tr>
			      <td>1xRGBD Only</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
				  <td>&cross;</td>
			      <td>&cross;</td>
			    </tr>
				<tr>
			      <td>1xRGBD + 2d Residuals</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&#10003;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
				  <td>&cross;</td>
			      <td>&cross;</td>
			    </tr>
				<tr>
			      <td>1xRGBD + 3d Residuals</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
				  <td>&cross;</td>
			      <td>&cross;</td>
			    </tr>
				<!-- <tr>
			      <td>1xRGBD + All Residuals</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&#10003;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
				  <td>&cross;</td>
			      <td>&cross;</td>
			    </tr> -->
				<tr>
			      <td>2xRGBD Only</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
				  <td>&#10003;</td>
			      <td>&#10003;</td>
			    </tr>
				<tr>
			      <td>2xRGBD + 2d Residuals</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&#10003;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
				  <td>&#10003;</td>
			      <td>&#10003;</td>
			    </tr>
				<tr>
			      <td>2xRGBD + 3d Residuals</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
				  <td>&#10003;</td>
			      <td>&#10003;</td>
			    </tr>
				<!-- <tr>
			      <td>2xRGBD + All Residuals</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&#10003;</td>
			      <td>&cross;</td>
			      <td>&cross;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
				  <td>&#10003;</td>
			      <td>&#10003;</td>
			    </tr> -->
				<tr>
			      <td>2xRGBD Full</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
			      <td>&#10003;</td>
				  <td>&#10003;</td>
			      <td>&#10003;</td>
			    </tr>
			  </tbody>
			</table>
			<br>


			<div class="caption">
			  <span>Table 2. Comparison of model performance</span>
			</div>

			<table border="1" style="width:100%; text-align:center; border-collapse:collapse;">
			  <thead>
			    <tr>
			      <th>Model</th>
			      <th>Dice coefficient</th>
			      <th>Recall</th>
				  <th>Precision</th>
			    </tr>
				<tr>
			      <td>Non-learned</td>
			      <td>0.475</td>
			      <td>0.713</td>
			      <td>0.320</td>
				</tr>
				<tr>
			      <td>1xRGBD Only</td>
			      <td>0.440</td>
			      <td><strong>0.973</strong></td>
			      <td>0.510</td>
				</tr>
				<tr>
			      <td>1xRGBD + 2D Residuals</td>
			      <td>0.499</td>
			      <td>0.934</td>
			      <td>0.499</td>
				</tr>
				<tr>
			      <td>1xRGBD + 3D Residuals</td>
			      <td>0.478</td>
			      <td>0.886</td>
			      <td>0.424</td>
				</tr>
				<!-- <tr>
			      <td>1xRGBD + All Residuals</td>
			      <td>X</td>
			      <td>X</td>
			      <td>X</td>
				</tr> -->
				<tr>
			      <td>2xRGBD Only</td>
			      <td>0.491</td>
			      <td>0.908</td>
			      <td>0.523</td>
				</tr>
				<tr>
			      <td>2xRGBD + 2D Residuals</td>
			      <td>0.474</td>
			      <td>0.936</td>
			      <td>0.468</td>
				</tr>
				<tr>
			      <td>2xRGBD + 3D Residuals</td>
			      <td>0.464</td>
			      <td>0.940</td>
			      <td>0.462</td>
				</tr>
				<!-- <tr>
			      <td>2xRGBD + All Residuals</td>
			      <td>X</td>
			      <td>X</td>
			      <td>X</td>
				</tr> -->
				<tr>
			      <td>2xRGBD Full</td>
			      <td><strong>0.528</strong></td>
			      <td>0.904</td>
			      <td><strong>0.538</strong></td>
				</tr>
			  </thead>
			</table>
					
			<br>

			<p>


			</p>

			<br>
			
			<figure>
				<video class='my-video' loop autoplay muted controls style="width: 800px; max-width: 100%;">
					<source src="./videos/full_eval.mp4" type="video/mp4">
				</video>

				<div class="caption">
				<span>Video 2. Example output sequence from the evaluation dataset (2xRGBD Full)</span>
				</div>
			</figure>

			The full evaluation video for 2xRGBD Full can be found <a href="https://drive.google.com/file/d/1YNJtrirsRWK8CpCwfgdxLsONLG5R5X97/view?usp=sharing">here</a>.

			</div>
			<div class="margin-right-block">
			</div>
		</div>

		<div class="content-margin-container" id="conclusion">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">

				<h1> Conclusion </h1>
				<p>
				We observed significantly better results with the model-based approaches when compared to the original non-learned algorithm in both precision and recall,
				achieving our objective of real-time dynamic object detection with high accuracy.


				<p>
				We note that the recall of the models was significantly higher than their precision. 
				Based on examination of the videos, we reached the conclusion that this is due to noisy frames with many false positives
				in which the RAFT and geometric flow significantly diverge, most likely due to noisy odometry or depth. 
				This leads the whole frame to become 
				noisy, thus possibly detecting many more objects in the image. In our training and evaluation data 
				there are at most 2 objects, thus there is only so many false negatives that one can incur per frame,
				and our models tend to be aggressive in their detections.	
				
				
				<p>

				Interestingly, and perhaps somewhat expectedly, the model that only uses RGBD data without any flow information already outperforms the non-learned method by a large margin.
				Therefore, much of this improvement can probably be attributed to the model learning how to detect robots (moving or not) from the RGB and depth data alone.
				Still, the model that used all of the inputs outperformed all other methods in Dice coefficient and precision, suggesting
				that the optical flow residuals do provide useful information for the task of dynamic object detection. These specific improved metrics suggest that 
				the model is better able to distinguish between moving and static robots, thus slightly reducing false positives (though not significantly,
				because of the high noise ratio), which reflects the value of the optical flow information. To better understand this behavior, 
				we would need to collect and train on data in which the robots do not move a majority of the time.

				<p>
				As seen in Figure 2, the model also generalizes to humans, despite never having seen any human data during training, suggesting highly promising generalization capabilities.
				<figure>
					<img src="./images/human.png"
					     alt="Human being detected."
					     >

						 <div class="caption">
			  <span>Figure 2: a moving human being detected</span>
			</div>
				</figure>
			

				<p> We believe that the performance of our models could be further improved by providing them with additional training data and more training time,
					especially due to the high false positive rate.
				Given how hard it is to generate meaningful training data, data augmentation methods
				should be explored in order to allow for creating larger datasets from pre-existing data. 
				To leverage the additional data, a more complex model, possibly using attention mechanisms, could be explored as well. However,
				model complexity would likely come at the cost of inference speed.
				</p>

				<p>
					Another area of improvement could be to improve temporal consistency.
					Currently, each sequence of two frames is treated independently, however, in practice, temporal consistency could provide significant improvements.
					Methods such as LSTMs or 3D convolutions could be explored for this purpose. Again, these methods would likely slow the method down significantly.
				</p>

				<p> 

				In conclusion, we believe that our method provides a lightweight and efficient solution for dynamic object detection.
				Future work could explore more complex models and temporal consistency in order to further improve performance, but 
				over 90% recall and over 50% precision is a promising result for a model trained from scratch with limited data.
			</p>
			</div>
			<div class="margin-right-block">
			</div>
		</div>
		
		<div class="content-margin-container" id="codebase">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">
				<h1> Codebase </h1>
				<p>See <a href="https://github.com/AndyLi23/dynamic-object-detection" target="_blank">[dynamic-object-detection on Github]</a> for our implementation.
					Instructions for downloading our data and running the algorithm and evaluations are in the README. 
					
					Note that this repository only contains the code responsible for the dynamic object detection side of things. 
					Our fork of the U-Net repository is linked as a submodule and contains more training and evaluation information.
				</p>
			</div>
			<div class="margin-right-block">
			</div>
		</div>
		
		<div class="content-margin-container" id="Acknowledgements">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">
				<h1> Acknowledgements </h1>
				We would like to thank Professor Jonathan How for his guidance throughout this project and for generously providing access to the resources that we utilized.
				Special thanks as well to Lucas Jia for assisting us with physical experiments, data collection and providing us with hardware powerful enough to train our model.
			</div>
			<div class="margin-right-block">
			</div>
		</div>
		
		<div class="content-margin-container" id="citations">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">
				<div class='citation' id="references" style="height:auto"><br>
					<span style="font-size:16px">References:</span><br><br>
					<a id="ref_1"></a>[1] <a href="https://arxiv.org/abs/2404.12389">Moving Object Segmentation: All You Need Is SAM (and Flow)</a>, Junyu Xie, Charig Yang, Weidi Xie, and Andrew Zisserman, 2024<br><br>
					<a id="ref_2"></a>[2] <a href="https://arxiv.org/abs/2304.02643">Segment Anything</a>, Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll√°r, and Ross Girshick, 2023<br><br>
					<a id="ref_3"></a>[3] <a href="https://arxiv.org/abs/2408.00714">SAM 2: Segment Anything in Images and Videos</a>, Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, and 14 others, 2024<br><br>
					<a id="ref_4"></a>[4] <a href="https://arxiv.org/abs/2511.16719">SAM 3: Segment Anything with Concepts</a>, Nicolas Carion, Laura Gustafson, Yuan-Ting Hu, Shoubhik Debnath, Ronghang Hu, Didac Suris, Chaitanya Ryali, Kalyan Vasudev Alwala, Haitham Khedr, and 13 others, 2025<br><br>
					<a id="ref_5"></a>[5] <a href="https://arxiv.org/abs/2507.19790">DepthFlow: Exploiting Depth-Flow Structural Correlations for Unsupervised Video Object Segmentation</a>, Suhwan Cho, Minhyeok Lee, Jungho Lee, Donghyeong Kim, and Sangyoun Lee, 2025<br><br>
					<a id="ref_6"></a>[6] <a href="https://arxiv.org/abs/2003.12039">RAFT: Recurrent All-Pairs Field Transforms for Optical Flow</a>, Zachary Teed and Jia Deng, 2020<br><br>

					<a id="ref_7"></a>[7] <a href="https://arxiv.org/abs/1505.04597">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>, Olaf Ronneberger, Philipp Fischer, and Thomas Brox, 2015<br><br>

					<a id="ref_8"></a>[8] <a href="https://github.com/milesial/Pytorch-UNet">Pytorch-UNet: PyTorch implementation of the U-Net for image semantic segmentation with high quality images</a>, Milesial, 2024<br><br>

					<a id="ref_9"></a>[9] P. L. et al., <a href="https://arxiv.org/abs/1912.00438">"RST-MODNet: Real-time Spatio-temporal Moving Object Detection for Autonomous Driving,"</a> 2019.<br><br>

					<a id="ref_10"></a>[10] N. Huang, W. Zheng, C. Xu, K. Keutzer, S. Zhang, A. Kanazawa, and Q. Wang, <a href="https://arxiv.org/abs/2503.22268">"Segment Any Motion in Videos,"</a> *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2025.<br><br>
					<a id="ref_11"></a>[11] M. Oquab et al., <a href="https://arxiv.org/abs/2304.07193">"DINOv2: Learning Robust Visual Features without Supervision,"</a> *Trans. Mach. Learn. Res. (TMLR)*, 2024.<br><br>

				</div>
			</div>
			<div class="margin-right-block">
				<!-- margin notes for reference block here -->
			</div>
		</div>


		<div class="content-margin-container" id="appendix">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block">
				<h1> Appendix </h1>
				This analysis of the difference between the RAFT optical flow and geometric optical flow is reproduced from the previous project.
				
				<blockquote class="centered-quote">
				We obtain our flow residuals by computing the difference between the RAFT optical flow and the "geometric optical flow", or optical flow
				resulting solely from camera-induced scene flow. 
				<br><br>
				For each pixel \( [u, v] \) with corresponding depth \( d \) in the first frame, we first unproject the pixel coordinates into 3D space using the depth data and camera intrinsics:
				\[ p_1=d K^{-1} [u, v, 1]^T \]
				We assume that \( p_1 \) is static in the 3D scene. We transform the 3D point \( p_1 \), which is in the reference frame of the camera at frame 1, into the reference frame of the camera at frame 2. Given that 
				the robot pose is \( T_{world}^{r1} \) at frame 1 and \( T_{world}^{r2} \) at frame 2, and that the transform between the odometry frame and camera frame is \( T_{odom}^{cam} \), 
				we can transform the point as follows:

				\[ T_{r2}^{r1} = (T_{world}^{r2})^{-1} \cdot T_{world}^{r1} \]
				\[ p_2 = (T_{odom}^{cam})^{-1} \cdot T_{r2}^{r1} \cdot T_{odom}^{cam} \cdot [p_1^T \dots 1]^T \]

				Finally, we project the point \( p_2 \) into the image plane to get its pixel location in the second frame:
				\[ [u', v'] = [f_x \frac{p_{2,x}}{p_{2,z}} + c_x, f_y \frac{p_{2,y}}{p_{2,z}} + c_y] \]
				where \( f_x, f_y \) are the focal lengths of the camera and \( c_x, c_y \) are the coordinates of the camera center in the image plane.

				Then, \( [u', v'] - [u, v] \) gives us the geometric optical flow at \( [u, v] \) in frame 1.

				The key observation is the assumption that \( p_1 \) is static in the 3D scene. If \( p_1 \) moves \( \delta = [\delta_x, \delta_y, \delta_z] \) (in the camera reference frame at frame 2) 
				between frame 1 and frame 2, we get the following image projection at frame 2:

				\[ [u'_{dynamic}, v'_{dynamic}] = [f_x \frac{p_{2,x} + \delta_x}{p_{2,z} + \delta_z} + c_x, f_y \frac{p_{2,y} + \delta_y}{p_{2,z} + \delta_z} + c_y] \]

				Since RAFT estimates the actual observed optical flow from frame 1 to frame 2, it will return an optical flow close to \( [u'_{dynamic}, v'_{dynamic}] - [u, v] \) at \( [u, v] \).
				Then, the residual (RAFT minus geometric flow) at \( [u, v] \) is \( [u'_{dynamic}, v'_{dynamic}] - [u', v'] \).
				
				<p> Because we have the depth data for both frame 1 and frame 2, in theory we can compute the full 3D movement
					of each pixel between frames.
				Given RAFT flow \( [\delta u_{raft}, \delta v_{raft}] \) at pixel \( [u, v] \), we can compute the 3D residual as follows:
					\[ p_2 = (T_{odom}^{cam})^{-1} \cdot T_{r2}^{r1} \cdot T_{odom}^{cam} \cdot [p_1^T \dots 1]^T + [u_{raft}, v_{raft}, 0]^T \]
					\[ p_{2, raft} = d_2 K^{-1} [u + \delta u_{raft}, v + \delta v_{raft}, 1]^T \]
					\[ \delta \approx p_{2, raft} - p_2 \]

					where \( d_2 \) is the depth of pixel \( [u + \delta u_{raft}, v + \delta v_{raft}] \) in frame 2. This method works by
					assuming that RAFT correctly predicts pixel correspondences, in which case the 3D point corresponding to pixel
					\( [u + \delta u_{raft}, v + \delta v_{raft}] \) in frame 2 is the same as the 3D point corresponding to pixel \( [u, v] \) in frame 1.
					Since \( p_2 \) accounts for the camera motion between frames but not the motion of the point itself, while RAFT accounts
					for both, the difference is the approximate 3D motion of the point between frames.

					<p> Although we could theoretically recover the depth of \( [u'_{dynamic}, v'_{dynamic}] \) from the depth data for frame 2, in practice, noisy depth data and imperfect
				pixel associations make this a noisy process. Instead, we assume that the object maintains constant depth between frames (\( \delta_z = 0 \)) and use the residual to recover only \( [\delta_x, \delta_y] \):

				\[ [\delta_x, \delta_y] = p_{2,z} ([u'_{dynamic}, v'_{dynamic}] - [u', v']) \cdot [\frac{1}{f_x}, \frac{1}{f_y}] \] 

				This assumption works reasonably well in practice, although it is not effective at detecting objects moving along the camera Z axis.
				<br><br>
				Finally, given \( v=\| \delta_x, \delta_y \| \) as an estimate for the velocity of the 3D point corresponding to each pixel, we can mask areas of the image with
				\( v \) higher than a set threshold to obtain masks of dynamic objects in frame 1.
				</blockquote>
			</div>
			<div class="margin-right-block">
			</div>
		</div>


	</body>

</html>
